{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# add path to Data dir\n",
    "sys.path.insert(1, '../Data/')\n",
    "from data_utils import get_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../Data/db.npz'\n",
    "with np.load(data_path, allow_pickle = True) as f:\n",
    "    param_labels = f['param_labels'] # last one is cosmo sigma_8\n",
    "    redshifts = f['redshifts']\n",
    "    LF_zs = f['LF_zs']\n",
    "    M_UV = f['M_UV']\n",
    "    N_params = len(param_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the data using `DataLoader` objects that supply batches of data. We split the database into three parts: \n",
    "- training: database used to update the weights of the NN (most of the data)\n",
    "- validation: database used to assess the NN's performance during the training (e.g. to avoid over/under fitting, to change the learning rate).\n",
    "- test: database used to assess the NN's performance after the training is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, valid_dataloader, test_dataloader, norms = get_dataloaders(f_train=0.8, f_valid=0.1, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tb_bias, Tb_scale,Ts_bias, Ts_scale,UVLFs_bias, UVLFs_scale,tau_bias, tau_scale = norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a class that will produce a fully connected neural network for us to train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Feed forward network architecture with optional activation at the end.\n",
    "    Parameters\n",
    "    ----------\n",
    "    dims : array-like\n",
    "        Dictates the network number of nodes and number of layers: \n",
    "        dims = [size of input, \n",
    "            number of nodes in hidden layer 1, ..., \n",
    "            number of nodes in hidden layer N, \n",
    "            size of output]\n",
    "    act : callable, optional\n",
    "        The activation function that goes between each nn.Linear layer.\n",
    "        Default is nn.ReLU()\n",
    "    final_act : callable, optional\n",
    "        The activation function that goes at the very end of the network.\n",
    "        Default is None.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dims, act=nn.ReLU(), final_act=None):\n",
    "        super(FeedForward, self).__init__()\n",
    "        num = len(dims) - 2\n",
    "        net = nn.Sequential()\n",
    "        for i in range(num):\n",
    "            net.append(nn.Linear(dims[i], dims[i+1]))\n",
    "            net.append(act)\n",
    "        i += 1\n",
    "        net.append(nn.Linear(dims[i], dims[i+1]))\n",
    "                    \n",
    "        if final_act is not None:\n",
    "            net.append(final_act)\n",
    "        self.net = net\n",
    "                                 \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Emulator(nn.Module):\n",
    "    def __init__(self,params_dict):\n",
    "        super().__init__()\n",
    "        self.xhi = FeedForward(dims=params_dict['xhi']['dims'], final_act = params_dict['xhi']['final_act'])\n",
    "        self.tb_shape = 1\n",
    "        self.tb = FeedForward(dims=params_dict['tb']['dims'], final_act = params_dict['tb']['final_act'])\n",
    "        self.ts = FeedForward(dims=params_dict['ts']['dims'], final_act = params_dict['ts']['final_act'])\n",
    "        self.lfs6 = FeedForward(dims = params_dict['lfs']['dims'], final_act = params_dict['lfs']['final_act'])\n",
    "        self.lfs7 = FeedForward(dims = params_dict['lfs']['dims'], final_act = params_dict['lfs']['final_act'])\n",
    "        self.lfs8 = FeedForward(dims = params_dict['lfs']['dims'], final_act = params_dict['lfs']['final_act'])\n",
    "        self.lfs9 = FeedForward(dims = params_dict['lfs']['dims'], final_act = params_dict['lfs']['final_act'])\n",
    "        self.lfs10 = FeedForward(dims = params_dict['lfs']['dims'], final_act = params_dict['lfs']['final_act'])\n",
    "        self.lfs12 = FeedForward(dims = params_dict['lfs']['dims'], final_act = params_dict['lfs']['final_act'])\n",
    "        self.lfs15 = FeedForward(dims = params_dict['lfs']['dims'], final_act = params_dict['lfs']['final_act'])\n",
    "        self.tau = FeedForward(dims = params_dict['tau']['dims'], final_act = params_dict['tau']['final_act'])\n",
    "    def forward(self, theta):\n",
    "        xhi_pred = self.xhi(theta)\n",
    "        tb_pred = self.tb(theta)\n",
    "        ts_pred = self.ts(theta)\n",
    "        lfs_pred = torch.cat([self.lfs6(theta)[...,np.newaxis],\n",
    "                              self.lfs7(theta)[...,np.newaxis],\n",
    "                              self.lfs8(theta)[...,np.newaxis],\n",
    "                              self.lfs9(theta)[...,np.newaxis],\n",
    "                              self.lfs10(theta)[...,np.newaxis],\n",
    "                              self.lfs12(theta)[...,np.newaxis],\n",
    "                              self.lfs15(theta)[...,np.newaxis]], axis = -1)\n",
    "        tau_pred = self.tau(theta)\n",
    "        return xhi_pred.squeeze(), tb_pred.squeeze(), ts_pred.squeeze(), lfs_pred.squeeze(), tau_pred.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "emu_params = {'lfs':{'dims':[N_params, 1024,1024,1024,len(M_UV)], 'final_act':nn.Sigmoid()},\n",
    "              'tau':{'dims':[N_params,128,128,128,128,1], 'final_act':nn.Sigmoid()},\n",
    "              'ts':{'dims':[N_params, 1024,1024,len(redshifts)], 'final_act':nn.Sigmoid()},\n",
    "              'tb':{'dims':[N_params, 1024,1024,len(redshifts)], 'final_act':nn.Sigmoid()},\n",
    "              'xhi':{'dims':[N_params, 1024,1024,len(redshifts)], 'final_act':nn.Sigmoid()},\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Emulator(\n",
       "  (xhi): FeedForward(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=11, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=1024, out_features=93, bias=True)\n",
       "      (5): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (tb): FeedForward(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=11, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=1024, out_features=93, bias=True)\n",
       "      (5): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (ts): FeedForward(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=11, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=1024, out_features=93, bias=True)\n",
       "      (5): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (lfs6): FeedForward(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=11, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=1024, out_features=45, bias=True)\n",
       "      (7): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (lfs7): FeedForward(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=11, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=1024, out_features=45, bias=True)\n",
       "      (7): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (lfs8): FeedForward(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=11, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=1024, out_features=45, bias=True)\n",
       "      (7): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (lfs9): FeedForward(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=11, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=1024, out_features=45, bias=True)\n",
       "      (7): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (lfs10): FeedForward(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=11, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=1024, out_features=45, bias=True)\n",
       "      (7): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (lfs12): FeedForward(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=11, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=1024, out_features=45, bias=True)\n",
       "      (7): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (lfs15): FeedForward(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=11, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=1024, out_features=45, bias=True)\n",
       "      (7): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (tau): FeedForward(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=11, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (7): ReLU()\n",
       "      (8): Linear(in_features=128, out_features=1, bias=True)\n",
       "      (9): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Emulator(emu_params)\n",
    "#model.load_state_dict(torch.load(str(results_folder) + '/model_pt10'))\n",
    "model.float()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(list(model.parameters()), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the loss as a weighted sum of the loss of individual summaries:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathcal{L} = \\sum_{i=0}^4 w_i||\\vec{s}_{\\rm{pred, }i} - \\vec{s}_{\\rm{true, }i}||^2_2,\n",
    "\\end{equation*} \n",
    "\n",
    "where $i = 0,1,...,4$ labels each of the summary statistics we are learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(true, pred, loss_fnc = F.mse_loss,weights=None):\n",
    "    if weights is None:\n",
    "        weights = np.ones(len(true))\n",
    "    xhi_pred, tb_pred, ts_pred, lfs_pred, tau_pred = pred\n",
    "    xhi_true, tb_true, ts_true, lfs_true, tau_true = true\n",
    "    xhi_loss = loss_fnc(xhi_true, xhi_pred)\n",
    "    tb_loss = loss_fnc(tb_true, tb_pred)\n",
    "    ts_loss = loss_fnc(ts_true, ts_pred)\n",
    "    lfs_loss = loss_fnc(lfs_true, lfs_pred)\n",
    "    tau_loss = loss_fnc(tau_true, tau_pred)\n",
    "    \n",
    "    loss = weights[0] * xhi_loss + weights[1]*tb_loss + weights[2]*ts_loss + weights[3]*lfs_loss + weights[4]*tau_loss\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import train, validate, lr_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = 1000\n",
    "epoch = 0\n",
    "results_folder = 'results'\n",
    "epoch_vloss = []\n",
    "epoch_tloss = []\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size = 1, gamma=0.1)\n",
    "\n",
    "while epoch < nepochs:\n",
    "    tloss, model, optimizer = train(model, train_dataloader, optimizer, loss, epoch, device=device)\n",
    "    epoch_tloss.append(tloss)\n",
    "    vloss = validate(model, valid_dataloader, optimizer, loss, epoch, device)\n",
    "    epoch_vloss.append(vloss)\n",
    "    if epoch == 0:\n",
    "        plateau = 0\n",
    "    scheduler, plateau = lr_schedule(optimizer, epoch_vloss, plateau)\n",
    "    this_loss = epoch_vloss[-1]\n",
    "    if epoch < 5:\n",
    "        torch.save(model.state_dict(), str(results_folder) + '/model_'+str(epoch))\n",
    "    if (epoch >= 5 and this_loss <= np.sort(epoch_vlosses[:-1])[4]):\n",
    "        num = int(np.where(np.sort(epoch_vlosses) == this_loss)[0][0])\n",
    "        torch.save(model.state_dict(), str(results_folder) + '/model_'+str(num))\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
