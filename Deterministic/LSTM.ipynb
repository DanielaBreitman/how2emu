{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# add path to Data dir\n",
    "sys.path.insert(1, '../Data/')\n",
    "from data_utils import get_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data_path = '../Data/db.npz'\n",
    "with np.load(data_path, allow_pickle = True) as f:\n",
    "    param_labels = f['param_labels'] # last one is cosmo sigma_8\n",
    "    redshifts = f['redshifts']\n",
    "    LF_zs = f['LF_zs']\n",
    "    M_UV = f['M_UV']\n",
    "    N_params = len(param_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_dataloader, valid_dataloader, test_dataloader, norms = get_dataloaders(f_train=0.8, f_valid=0.1, batch_size=64, lstm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Tb_bias, Tb_scale,Ts_bias, Ts_scale,UVLFs_bias, UVLFs_scale,tau_bias, tau_scale = norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \"\"\"LSTM\"\"\"\n",
    "    def __init__(self,dims,num_layers=2, final_act=None, init=False):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=dims[0], hidden_size = dims[1], num_layers=num_layers, batch_first=True)\n",
    "        self.lin = nn.Linear(dims[1], dims[2])\n",
    "        self.final_act = final_act\n",
    "        if init:\n",
    "            self.lstm.apply(self._init_weights)\n",
    "            self.lin.apply(self._init_weights)\n",
    "            \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=1.0)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "\n",
    "    def forward(self,x):\n",
    "        output, hidden = self.lstm(x)\n",
    "        output = self.lin(output)\n",
    "        if self.final_act is not None:\n",
    "            output = self.final_act(output)\n",
    "        return output\n",
    "\n",
    "class Emulator(nn.Module):\n",
    "    def __init__(self,params_dict):\n",
    "        super().__init__()\n",
    "        self.xhi = LSTM(dims=params_dict['xhi']['dims'], num_layers = params_dict['xhi']['num_layers'], final_act = params_dict['xhi']['final_act'])\n",
    "        self.tb_shape = 1\n",
    "        self.tb = LSTM(dims=params_dict['tb']['dims'], num_layers = params_dict['tb']['num_layers'], final_act = params_dict['tb']['final_act'])\n",
    "        self.ts = LSTM(dims=params_dict['ts']['dims'], num_layers = params_dict['ts']['num_layers'], final_act = params_dict['ts']['final_act'])\n",
    "        self.lfs6 = LSTM(dims = params_dict['lfs']['dims'], num_layers = params_dict['lfs']['num_layers'], final_act = params_dict['lfs']['final_act'])\n",
    "        self.lfs7 = LSTM(dims = params_dict['lfs']['dims'], num_layers = params_dict['lfs']['num_layers'], final_act = params_dict['lfs']['final_act'])\n",
    "        self.lfs8 = LSTM(dims = params_dict['lfs']['dims'], num_layers = params_dict['lfs']['num_layers'], final_act = params_dict['lfs']['final_act'])\n",
    "        self.lfs9 = LSTM(dims = params_dict['lfs']['dims'], num_layers = params_dict['lfs']['num_layers'], final_act = params_dict['lfs']['final_act'])\n",
    "        self.lfs10 = LSTM(dims = params_dict['lfs']['dims'], num_layers = params_dict['lfs']['num_layers'], final_act = params_dict['lfs']['final_act'])\n",
    "        self.lfs12 = LSTM(dims = params_dict['lfs']['dims'], num_layers = params_dict['lfs']['num_layers'], final_act = params_dict['lfs']['final_act'])\n",
    "        self.lfs15 = LSTM(dims = params_dict['lfs']['dims'], num_layers = params_dict['lfs']['num_layers'], final_act = params_dict['lfs']['final_act'])\n",
    "        self.tau = LSTM(dims = params_dict['tau']['dims'], final_act = params_dict['tau']['final_act'])\n",
    "    def forward(self, theta):\n",
    "        xhi_pred = self.xhi(theta)\n",
    "        tb_pred = self.tb(theta)\n",
    "        ts_pred = self.ts(theta)\n",
    "        lfs_pred = torch.cat([self.lfs6(theta),\n",
    "                              self.lfs7(theta),\n",
    "                              self.lfs8(theta),\n",
    "                              self.lfs9(theta),\n",
    "                              self.lfs10(theta),\n",
    "                              self.lfs12(theta),\n",
    "                              self.lfs15(theta)], axis = -1)\n",
    "        tau_pred = self.tau(theta)\n",
    "        return xhi_pred.squeeze(), tb_pred.squeeze(), ts_pred.squeeze(), lfs_pred.squeeze(), tau_pred.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "emu_params = {'lfs':{'dims':[11, len(M_UV),1], 'num_layers':2, 'final_act':nn.Sigmoid()},\n",
    "              'tau':{'dims':[11,128,128,128,128,1], 'final_act':nn.Sigmoid()},\n",
    "              'ts':{'dims':[11, len(redshifts),1], 'num_layers':2, 'final_act':nn.Sigmoid()},\n",
    "              'tb':{'dims':[11, len(redshifts),1], 'num_layers':2, 'final_act':nn.Sigmoid()},\n",
    "              'xhi':{'dims':[11, len(redshifts),1], 'num_layers':2, 'final_act':nn.Sigmoid()},\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model = Emulator(emu_params)\n",
    "#model.load_state_dict(torch.load(str(results_folder) + '/model_pt10'))\n",
    "model.float()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(list(model.parameters()), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def loss(true, pred, loss_fnc = F.mse_loss,weights=None):\n",
    "    if weights is None:\n",
    "        weights = np.ones(len(true))\n",
    "    xhi_pred, tb_pred, ts_pred, lfs_pred, tau_pred = pred\n",
    "    xhi_true, tb_true, ts_true, lfs_true, tau_true = true\n",
    "    xhi_loss = loss_fnc(xhi_true, xhi_pred)\n",
    "    tb_loss = loss_fnc(tb_true, tb_pred)\n",
    "    ts_loss = loss_fnc(ts_true, ts_pred)\n",
    "    lfs_loss = loss_fnc(lfs_true, lfs_pred)\n",
    "    tau_loss = loss_fnc(tau_true, tau_pred)\n",
    "    \n",
    "    loss = weights[0] * xhi_loss + weights[1]*tb_loss + weights[2]*ts_loss + weights[3]*lfs_loss + weights[4]*tau_loss\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from training import train, validate, lr_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "nepochs = 1000\n",
    "epoch = 0\n",
    "results_folder = 'results'\n",
    "epoch_vloss = []\n",
    "epoch_tloss = []\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size = 1, gamma=0.1)\n",
    "\n",
    "while epoch < nepochs:\n",
    "    tloss, model, optimizer = train(model, train_dataloader, optimizer, loss, epoch, device=device)\n",
    "    epoch_tloss.append(tloss)\n",
    "    vloss = validate(model, valid_dataloader, optimizer, loss, epoch, device)\n",
    "    epoch_vloss.append(vloss)\n",
    "    if epoch == 0:\n",
    "        plateau = 0\n",
    "    scheduler, plateau = lr_schedule(optimizer, epoch_vloss, plateau)\n",
    "    this_loss = epoch_vloss[-1]\n",
    "    if epoch < 5:\n",
    "        torch.save(model.state_dict(), str(results_folder) + '/model_'+str(epoch))\n",
    "    if (epoch >= 5 and this_loss <= np.sort(epoch_vlosses[:-1])[4]):\n",
    "        num = int(np.where(np.sort(epoch_vlosses) == this_loss)[0][0])\n",
    "        torch.save(model.state_dict(), str(results_folder) + '/model_'+str(num))\n",
    "    epoch += 1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
